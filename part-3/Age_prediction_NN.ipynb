{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS760 Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1A5s3D300NXX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader,IterableDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oppScrData = pd.read_excel (r'sample_data/OppScrData.xlsx')"
      ],
      "metadata": {
        "id": "UBgfOi7VZgxm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_ct_data(oppScrData):\n",
        "    # Delete rows with empty values\n",
        "    ct_data= oppScrData[[\"L1_HU_BMD\", \"TAT Area (cm2)\", 'Total Body                Area EA (cm2)',\n",
        "       'VAT Area (cm2)', 'SAT Area (cm2)', 'VAT/SAT     Ratio', 'Muscle HU',\n",
        "       ' Muscle Area (cm2)', 'L3 SMI (cm2/m2)', 'AoCa        Agatston',\n",
        "       'Liver HU    (Median)', 'Age at CT']]\n",
        "    n = ct_data.shape[0]\n",
        "    preprocessed_ct_data = []\n",
        "    for i in range(n):\n",
        "        row = ct_data.loc[i]\n",
        "        ignore = False\n",
        "        for j in row:\n",
        "          if pd.isna(j) or j == ' ': # There is an empty string somewhere in Liver column\n",
        "            ignore = True\n",
        "            break\n",
        "        if not ignore:\n",
        "          preprocessed_ct_data.append(row)\n",
        "    return np.array(preprocessed_ct_data, dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "zT0tS-hvfVfC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_ct_data(ct_data):\n",
        "    n = ct_data.shape[1]    \n",
        "    for i in range(n-1):\n",
        "      ct_data[:,i] = (ct_data[:,i] - np.min(ct_data[:,i]))/(np.max(ct_data[:,i])- np.min(ct_data[:,i]))\n",
        "    return ct_data"
      ],
      "metadata": {
        "id": "tEz5RjTvgE1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oppScrData = pd.read_excel (r'sample_data/OppScrData.xlsx')  \n",
        "ct_data = clean_ct_data(oppScrData)\n",
        "ct_data= normalize_ct_data(ct_data)"
      ],
      "metadata": {
        "id": "_1B1GORmev3F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = ct_data[:,-1]\n",
        "x = (ct_data[:,:-1])"
      ],
      "metadata": {
        "id": "lSDGb9QFbwxy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)"
      ],
      "metadata": {
        "id": "hIFoIYPJbqtd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CT_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.ct_data = X\n",
        "        self.age = y \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ct_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ct_data = self.ct_data[idx]     \n",
        "        age = np.array(self.age[idx], dtype=np.float32)\n",
        "        return torch.from_numpy(ct_data), torch.from_numpy(age)"
      ],
      "metadata": {
        "id": "XjU44f9rGhH7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = CT_Dataset(X=X_train, y = y_train)\n",
        "train_set = DataLoader(a, batch_size=64, shuffle=True)\n",
        "# train_set = DataLoader(a)"
      ],
      "metadata": {
        "id": "9A8n-S3PQ06Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  return nn.Sequential(\n",
        "    nn.Linear(11, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 32),\n",
        "    nn.BatchNorm1d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 10),\n",
        "    nn.BatchNorm1d(10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 1)\n",
        "    )\n",
        "\n",
        "model = build_model()"
      ],
      "metadata": {
        "id": "5WR8JdUB_QxC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train_model(model, train_loader, criterion, T):\n",
        "  model.train()\n",
        "  opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "  # opt = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  for epoch in range(T): \n",
        "      running_loss = 0\n",
        "      for data in enumerate(train_loader):\n",
        "          ct_data, actual_age = data[1][0], data[1][1]\n",
        "          opt.zero_grad()\n",
        "          predicted_age = model(ct_data)      \n",
        "          loss = criterion(predicted_age, actual_age)\n",
        "          running_loss += loss.item()*train_loader.batch_size\n",
        "          loss.backward() \n",
        "          opt.step()\n",
        "          \n",
        "      print(\"​Train Epoch: \"+str(epoch) + \" Loss =\", str(running_loss))\n",
        "\n",
        "  model.train(mode=False)\n",
        "\n",
        "model = build_model()\n",
        "train_model(model, train_set, criterion, T = 100)"
      ],
      "metadata": {
        "id": "485ik2ZeAD_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d43bcb7-227f-4a28-9730-819b450ccf2b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([54])) that is different to the input size (torch.Size([54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "​Train Epoch: 0 Loss = 26202353.234375\n",
            "​Train Epoch: 1 Loss = 25431270.9375\n",
            "​Train Epoch: 2 Loss = 24184200.078125\n",
            "​Train Epoch: 3 Loss = 22284124.78125\n",
            "​Train Epoch: 4 Loss = 19834438.171875\n",
            "​Train Epoch: 5 Loss = 16999667.078125\n",
            "​Train Epoch: 6 Loss = 13991026.46875\n",
            "​Train Epoch: 7 Loss = 10995382.1328125\n",
            "​Train Epoch: 8 Loss = 8280743.13671875\n",
            "​Train Epoch: 9 Loss = 5977162.56640625\n",
            "​Train Epoch: 10 Loss = 4089965.994140625\n",
            "​Train Epoch: 11 Loss = 2719650.5361328125\n",
            "​Train Epoch: 12 Loss = 1756240.5205078125\n",
            "​Train Epoch: 13 Loss = 1157539.4174804688\n",
            "​Train Epoch: 14 Loss = 847777.6672363281\n",
            "​Train Epoch: 15 Loss = 641880.0187988281\n",
            "​Train Epoch: 16 Loss = 560993.1391601562\n",
            "​Train Epoch: 17 Loss = 511217.82958984375\n",
            "​Train Epoch: 18 Loss = 493452.88623046875\n",
            "​Train Epoch: 19 Loss = 485972.6652832031\n",
            "​Train Epoch: 20 Loss = 480865.49755859375\n",
            "​Train Epoch: 21 Loss = 476567.89306640625\n",
            "​Train Epoch: 22 Loss = 474310.6611328125\n",
            "​Train Epoch: 23 Loss = 474095.91955566406\n",
            "​Train Epoch: 24 Loss = 472687.6960449219\n",
            "​Train Epoch: 25 Loss = 471052.81091308594\n",
            "​Train Epoch: 26 Loss = 471118.5362548828\n",
            "​Train Epoch: 27 Loss = 469336.24475097656\n",
            "​Train Epoch: 28 Loss = 470821.72021484375\n",
            "​Train Epoch: 29 Loss = 470956.5153808594\n",
            "​Train Epoch: 30 Loss = 470813.6672363281\n",
            "​Train Epoch: 31 Loss = 471213.587890625\n",
            "​Train Epoch: 32 Loss = 469160.2380371094\n",
            "​Train Epoch: 33 Loss = 469626.7606201172\n",
            "​Train Epoch: 34 Loss = 469582.8272705078\n",
            "​Train Epoch: 35 Loss = 469317.6867675781\n",
            "​Train Epoch: 36 Loss = 469717.396484375\n",
            "​Train Epoch: 37 Loss = 469516.75244140625\n",
            "​Train Epoch: 38 Loss = 470540.38525390625\n",
            "​Train Epoch: 39 Loss = 470166.1248779297\n",
            "​Train Epoch: 40 Loss = 469845.2813720703\n",
            "​Train Epoch: 41 Loss = 470690.25842285156\n",
            "​Train Epoch: 42 Loss = 468311.83056640625\n",
            "​Train Epoch: 43 Loss = 469725.6848144531\n",
            "​Train Epoch: 44 Loss = 469399.0098876953\n",
            "​Train Epoch: 45 Loss = 469330.3026123047\n",
            "​Train Epoch: 46 Loss = 470588.7971191406\n",
            "​Train Epoch: 47 Loss = 468287.35009765625\n",
            "​Train Epoch: 48 Loss = 469128.2449951172\n",
            "​Train Epoch: 49 Loss = 469132.97265625\n",
            "​Train Epoch: 50 Loss = 469344.8701171875\n",
            "​Train Epoch: 51 Loss = 468741.54931640625\n",
            "​Train Epoch: 52 Loss = 469018.13671875\n",
            "​Train Epoch: 53 Loss = 468869.3094482422\n",
            "​Train Epoch: 54 Loss = 469672.8879394531\n",
            "​Train Epoch: 55 Loss = 469761.2215576172\n",
            "​Train Epoch: 56 Loss = 469461.7902832031\n",
            "​Train Epoch: 57 Loss = 468926.7839355469\n",
            "​Train Epoch: 58 Loss = 467919.61682128906\n",
            "​Train Epoch: 59 Loss = 468614.61572265625\n",
            "​Train Epoch: 60 Loss = 468226.07360839844\n",
            "​Train Epoch: 61 Loss = 469056.20251464844\n",
            "​Train Epoch: 62 Loss = 468281.76599121094\n",
            "​Train Epoch: 63 Loss = 468598.8059082031\n",
            "​Train Epoch: 64 Loss = 468054.5059814453\n",
            "​Train Epoch: 65 Loss = 467775.3640136719\n",
            "​Train Epoch: 66 Loss = 467564.46130371094\n",
            "​Train Epoch: 67 Loss = 468114.9315185547\n",
            "​Train Epoch: 68 Loss = 470057.17028808594\n",
            "​Train Epoch: 69 Loss = 468274.4461669922\n",
            "​Train Epoch: 70 Loss = 468441.24670410156\n",
            "​Train Epoch: 71 Loss = 468223.759765625\n",
            "​Train Epoch: 72 Loss = 467734.4870605469\n",
            "​Train Epoch: 73 Loss = 468134.7811279297\n",
            "​Train Epoch: 74 Loss = 467552.62341308594\n",
            "​Train Epoch: 75 Loss = 467972.0617675781\n",
            "​Train Epoch: 76 Loss = 468804.46350097656\n",
            "​Train Epoch: 77 Loss = 468185.15478515625\n",
            "​Train Epoch: 78 Loss = 467956.14782714844\n",
            "​Train Epoch: 79 Loss = 467936.6190185547\n",
            "​Train Epoch: 80 Loss = 468163.91931152344\n",
            "​Train Epoch: 81 Loss = 467585.60974121094\n",
            "​Train Epoch: 82 Loss = 468405.3757324219\n",
            "​Train Epoch: 83 Loss = 467846.20349121094\n",
            "​Train Epoch: 84 Loss = 468571.61071777344\n",
            "​Train Epoch: 85 Loss = 467296.7052001953\n",
            "​Train Epoch: 86 Loss = 468513.609375\n",
            "​Train Epoch: 87 Loss = 467640.4221191406\n",
            "​Train Epoch: 88 Loss = 467308.23693847656\n",
            "​Train Epoch: 89 Loss = 467519.9715576172\n",
            "​Train Epoch: 90 Loss = 467643.53088378906\n",
            "​Train Epoch: 91 Loss = 467060.3728027344\n",
            "​Train Epoch: 92 Loss = 468056.8642578125\n",
            "​Train Epoch: 93 Loss = 467383.6298828125\n",
            "​Train Epoch: 94 Loss = 467769.4562988281\n",
            "​Train Epoch: 95 Loss = 467507.5598144531\n",
            "​Train Epoch: 96 Loss = 467161.1561279297\n",
            "​Train Epoch: 97 Loss = 467880.4384765625\n",
            "​Train Epoch: 98 Loss = 467551.1833496094\n",
            "​Train Epoch: 99 Loss = 467335.8088378906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = CT_Dataset(X=X_test, y = y_test)\n",
        "test_set = DataLoader(b, batch_size=64)\n",
        "# train_set = DataLoader(a)"
      ],
      "metadata": {
        "id": "WHHHQ7dzNrwH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "  model.eval()\n",
        "  running_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for data in enumerate(test_loader):\n",
        "          ct_data, actual_age = data[1][0], data[1][1]\n",
        "          predicted_age = model(ct_data)\n",
        "          loss = criterion(predicted_age, actual_age)\n",
        "          running_loss += loss.item()\n",
        "  return running_loss\n",
        "\n",
        "evaluate_model(model, test_set, criterion)"
      ],
      "metadata": {
        "id": "c-1z56dnZcFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f88d0c-39d1-47f4-f7ec-86742357af7a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "911.4797096252441"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EMDTR4K7Ox4w"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}